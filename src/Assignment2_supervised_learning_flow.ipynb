{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec202ada",
   "metadata": {},
   "source": [
    "# Assignment2 - Supervised Learning flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a2810",
   "metadata": {},
   "source": [
    "# Part 1 - Student details:\n",
    "* Please write the First Name and last 4 digits of the i.d. for each student. For example:\n",
    "<pre>Israel 9812</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca16486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student 1: Shlomo 6825\n",
    "# student 2: Eden 9994\n",
    "# student 3: Shira 3972"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67c2ee-87c8-499c-a04f-1853c332f51d",
   "metadata": {},
   "source": [
    "## Part 2 - Initial Preparations \n",
    "You could add as many code cells as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29b3454-b568-4614-8017-f15b3c59fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import zscore, stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b47fe56-d611-4b28-be92-673db4d56400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e27610-b640-4db0-80c9-789b5f5fae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first row of train DF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.37</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.36</td>\n",
       "      <td>10.6</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.82</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.82</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.88</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.44</td>\n",
       "      <td>415.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.16</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.15</td>\n",
       "      <td>21.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.68</td>\n",
       "      <td>830.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    12.08        1.83  2.32               18.5       81.0           1.60   \n",
       "1    13.58        2.58  2.69               24.5      105.0           1.55   \n",
       "2    12.37        0.94  1.36               10.6       88.0           1.98   \n",
       "3    11.82        1.72  1.88               19.5       86.0           2.50   \n",
       "4    13.16        3.57  2.15               21.0      102.0           1.50   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        1.50                  0.52             1.64             2.40  1.08   \n",
       "1        0.84                  0.39             1.54             8.66  0.74   \n",
       "2        0.57                  0.28             0.42             1.95  1.05   \n",
       "3        1.64                  0.37             1.42             2.06  0.94   \n",
       "4        0.55                  0.43             1.30             4.00  0.60   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          2.27    480.0       1  \n",
       "1                          1.80    750.0       2  \n",
       "2                          1.82    520.0       1  \n",
       "3                          2.44    415.0       1  \n",
       "4                          1.68    830.0       2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = load_file(r\"wine_train.csv\")\n",
    "df_test = load_file(r\"wine_test.csv\")\n",
    "#need to change the path to be rlevant to \n",
    "print(\"5 first row of train DF\")\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271cb20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first row of test DF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.17</td>\n",
       "      <td>5.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.48</td>\n",
       "      <td>725.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.03</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.20</td>\n",
       "      <td>21.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.87</td>\n",
       "      <td>407.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    13.78        2.76  2.30               22.0       90.0           1.35   \n",
       "1    14.02        1.68  2.21               16.0       96.0           2.65   \n",
       "2    13.17        5.19  2.32               22.0       93.0           1.74   \n",
       "3    11.03        1.51  2.20               21.5       85.0           2.46   \n",
       "4    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        0.68                  0.41             1.03             9.58  0.70   \n",
       "1        2.33                  0.26             1.98             4.70  1.04   \n",
       "2        0.63                  0.61             1.55             7.90  0.60   \n",
       "3        2.17                  0.52             2.01             1.90  1.71   \n",
       "4        0.75                  0.43             1.41             7.30  0.70   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          1.68    615.0       2  \n",
       "1                          3.59   1035.0       0  \n",
       "2                          1.48    725.0       2  \n",
       "3                          2.87    407.0       1  \n",
       "4                          1.56    750.0       2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 first row of test DF\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6baf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142 entries, 0 to 141\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       142 non-null    float64\n",
      " 1   malic_acid                    142 non-null    float64\n",
      " 2   ash                           142 non-null    float64\n",
      " 3   alcalinity_of_ash             142 non-null    float64\n",
      " 4   magnesium                     142 non-null    float64\n",
      " 5   total_phenols                 142 non-null    float64\n",
      " 6   flavanoids                    142 non-null    float64\n",
      " 7   nonflavanoid_phenols          142 non-null    float64\n",
      " 8   proanthocyanins               142 non-null    float64\n",
      " 9   color_intensity               142 non-null    float64\n",
      " 10  hue                           142 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  142 non-null    float64\n",
      " 12  proline                       142 non-null    float64\n",
      " 13  target                        142 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 15.7 KB\n"
     ]
    }
   ],
   "source": [
    "#We will perform an info function that checks the amount of data and whether there are any missing values\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c39734-1a20-4291-9571-4cd09d04b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 142 rows and we don't have any null value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e693046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to X_train and y_train\n",
    "X_train = df_train.drop(\"target\",axis=1)\n",
    "y_train =df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccacf933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Checking whether there is duplicate rows\n",
    "print(X_train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "300beb42-57f8-410b-b176-3f4816b1d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't have no duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of distribution according to z_score\n",
    "z_scores = X_train.apply(zscore)\n",
    "z_scores.hist(bins=20, figsize=(12, 8), color='darkred', edgecolor='black')\n",
    "plt.suptitle('Histograms of Z-Scores for Each Feature', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Presentation of a table according to Pearson's correlation coefficient between different features\n",
    "correlation_matrix = df_train.corr()\n",
    "(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb51aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Presentation of a heatmap according to above table\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='black',\n",
    "    square=True,\n",
    "    cbar_kws={'shrink': .75}\n",
    ")\n",
    "plt.title('Pearson Correlation Matrix', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da781fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to extract the features with coefficient more then 0.75.(abs(0.75))\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        if correlation_matrix.iloc[i,j]>0.75 :\n",
    "            print (\"in place (\" + str(i) + \",\" + str(j)  + \") we have high value:\"+ str(correlation_matrix.iloc[i,j]))\n",
    "        if correlation_matrix.iloc[i,j]<-0.75 :\n",
    "            print (\"in place (\" + str(i) + \",\" + str(j)  + \") we have low value:\"+ str(correlation_matrix.iloc[i,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the outlier data by z_score\n",
    "z_scores = X_train.apply(zscore)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=z_scores, orient='h', palette='coolwarm')\n",
    "plt.title('Boxplot of Z-Scores')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36672a",
   "metadata": {},
   "source": [
    "## Part 3 - Experiments\n",
    "You could add as many code cells as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad09a3-f6a7-437b-a8dd-4800651db99c",
   "metadata": {},
   "source": [
    "# *This is results without feature engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe09d51-1241-49e4-9101-9f66624e30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to X_train and y_train\n",
    "X_train = df_train.drop(\"target\",axis=1)\n",
    "y_train =df_train[\"target\"]\n",
    "data = list() #this dictionary will contain all the best score of the cross-validation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472f69c-497b-4b8d-a601-7d4702c8fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of knn algoritem in order to find the best hyperparameters\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],   \n",
    "    'weights': ['uniform', 'distance'], \n",
    "    'metric': ['euclidean', 'manhattan','minkowski','chebyshev'] \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring=f1) \n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "# Best hyperparameters and F1 score for KNN\n",
    "print(\"Best KNN Hyperparameters:\", knn_grid_search.best_params_)\n",
    "print(f\"Best KNN F1 Score: {knn_grid_search.best_score_:.4f}\")\n",
    "knn_without_feature_engineering_results = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"knn without feature engineering\", \"best score\": float(knn_grid_search.best_score_),'hyperparameter':knn_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9cdcf-af82-4d74-9135-49faa879b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of decision tree algoritem in order to find the best hyperparameters\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  \n",
    "    'max_depth': [3, 5, 10, None],    \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4]     \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring=f1)\n",
    "decision_tree_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best decision tree Hyperparameters: {decision_tree_grid_search.best_params_}\")\n",
    "best_model = decision_tree_grid_search.best_estimator_\n",
    "test_f1_score = f1_score(y_train, best_model.predict(X_train), average='weighted')\n",
    "print(f\"Best decision tree F1 Score: {test_f1_score:.4f}\")\n",
    "print(test_f1_score)\n",
    "decision_tree_without_feature_engineering_results = pd.DataFrame(decision_tree_grid_search.cv_results_)\n",
    "dict_ = {'model': \"decision tree without feature engineering\", \"best score\": float(test_f1_score),'hyperparameter':decision_tree_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82a222-4957-408e-9f7d-b4d6b5027da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944d9fb-5dec-45e6-af96-3f88be9be811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of NB algoritem in order to find the best hyperparameters\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "model = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=5)\n",
    "NB_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best NB Hyperparameters: {NB_grid_search.best_params_}\")\n",
    "print(f\"Best NB F1 Score: {NB_grid_search.best_score_:.4f}\")\n",
    "NB_without_feature_engineering_results = pd.DataFrame(NB_grid_search.cv_results_)\n",
    "dict_ = {'model': \"NB without feature engineering\", \"best score\": float(NB_grid_search.best_score_),'hyperparameter':NB_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf057afe-c39e-46b8-aa02-f4b36c89436f",
   "metadata": {},
   "source": [
    "# *feature engineering*: \n",
    "removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c123911-1966-4ccc-84a4-04aa7506824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_scores(df):\n",
    "    z_scores = (df - df.mean()) / df.std()\n",
    "    return z_scores\n",
    "\n",
    "z_scores = compute_z_scores(df_train.select_dtypes(include=['number']))\n",
    "\n",
    "# Set a threshold for Z-Score\n",
    "threshold = 2.5\n",
    "\n",
    "# Create a boolean mask for non-outlier rows\n",
    "mask = (abs(z_scores) < threshold).all(axis=1)\n",
    "\n",
    "# Filter the DataFrame to remove outliers\n",
    "filtered_df = df_train[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c83355-2946-43dd-841d-77502627a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = filtered_df.drop(\"target\",axis=1)\n",
    "y_train_filtered = filtered_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d83ba-f910-4eee-8dcb-a626314bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of knn algoritem in order to find the best hyperparameters without outlier\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],   \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'metric': ['euclidean', 'manhattan','minkowski','chebyshev']  \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring=f1)\n",
    "knn_grid_search.fit(X_filtered, y_train_filtered)\n",
    "# Best hyperparameters and F1 score for KNN\n",
    "print(\"Best KNN Hyperparameters:\", knn_grid_search.best_params_)\n",
    "print(f\"Best KNN F1 Score: {knn_grid_search.best_score_:.4f}\")\n",
    "knn_removing_outliers_results = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"knn remove outlier bs\", \"best score\": float(knn_grid_search.best_score_),'hyperparameter':knn_grid_search.best_params_ }\n",
    "data.append(dict_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035afcc-80a3-4ce4-95f8-839a04583577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of decision tree algoritem in order to find the best hyperparameters without outlier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  \n",
    "    'max_depth': [3, 5, 10, None],    \n",
    "    'min_samples_split': [2, 5, 10],   \n",
    "    'min_samples_leaf': [1, 2, 4]     \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring=f1)\n",
    "decision_tree_grid_search.fit(X_filtered, y_train_filtered)\n",
    "print(f\"Best decision tree Hyperparameters: {decision_tree_grid_search.best_params_}\")\n",
    "best_model = decision_tree_grid_search.best_estimator_\n",
    "test_f1_score = f1_score(y_train_filtered, best_model.predict(X_filtered), average='weighted')\n",
    "print(f\"Best decision tree F1 Score: {test_f1_score:.4f}\")\n",
    "decision_tree_outliers_results = pd.DataFrame(decision_tree_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"decision tree remove outlier bs\", \"best score\": float(test_f1_score),'hyperparameter':decision_tree_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7c635-9c67-4aa0-a04e-cfa2ce45c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of NB algoritem in order to find the best hyperparameters without outlier\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "model = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=5)\n",
    "NB_grid_search.fit(X_filtered, y_train_filtered)\n",
    "print(f\"Best NB Hyperparameters: {NB_grid_search.best_params_}\")\n",
    "print(f\"Best NB F1 Score: {NB_grid_search.best_score_:.4f}\")\n",
    "NB_outliers_results = pd.DataFrame(NB_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"NB remove outlier bs\", \"best score\": float(NB_grid_search.best_score_),'hyperparameter':NB_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc9333-4bb6-4365-b63c-1219a08a82c6",
   "metadata": {},
   "source": [
    "# *This is scaled results:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fc457-1280-406c-ad21-268fbe545439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e91ea-bb6e-4e7b-b8a5-5cb65df7bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of knn algoritem in order to find the best hyperparameters with scaling\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],   \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'metric': ['euclidean', 'manhattan','minkowski','chebyshev'] \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring=f1)\n",
    "knn_grid_search.fit(X_scaled, y_train)\n",
    "# Best hyperparameters and F1 score for KNN\n",
    "print(\"Best KNN Hyperparameters:\", knn_grid_search.best_params_)\n",
    "print(f\"Best KNN F1 Score: {knn_grid_search.best_score_:.4f}\")\n",
    "knn_scaled_results = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"knn scaled bs\", \"best score\": float(knn_grid_search.best_score_),'hyperparameter':knn_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f4dd9-95fe-458b-aa3e-076d31087cba",
   "metadata": {},
   "source": [
    "#Decision trees do not require scaling because they split data based on the order of feature values rather than their magnitudes, making the scale of the features irrelevant to their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935bd43-377d-48e2-ad6b-e2c7e4ff63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of NB algoritem in order to find the best hyperparameters with scaling\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "model = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=5)\n",
    "NB_grid_search.fit(X_scaled, y_train)\n",
    "print(f\"Best NB Hyperparameters: {NB_grid_search.best_params_}\")\n",
    "print(f\"Best NB F1 Score: {NB_grid_search.best_score_:.4f}\")\n",
    "NB_scaled_results = pd.DataFrame(NB_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"NB scaled bs\", \"best score\": float(NB_grid_search.best_score_),'hyperparameter':NB_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5ff9b-db8f-4e58-bf5b-5e2c95f4bf7a",
   "metadata": {},
   "source": [
    "# *Results for removing a dependent feature:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba827f68-8406-4a5a-a83d-65abc23e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In checking the correlation coefficient we found that there is a high correlation between the fifth and sixth features, so to optimize the data we will remove the fifth feature.\n",
    "df_removed_feature = df_train.drop(columns=['total_phenols']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8629e-dc2d-4482-a10b-b216f9b66453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of knn algoritem in order to find the best hyperparameters after removing feature with high correlation\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],   \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'metric': ['euclidean', 'manhattan','minkowski','chebyshev']  \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring=f1)\n",
    "knn_grid_search.fit(df_removed_feature, y_train)\n",
    "# Best hyperparameters and F1 score for KNN\n",
    "print(\"Best KNN Hyperparameters:\", knn_grid_search.best_params_)\n",
    "print(f\"Best KNN F1 Score: {knn_grid_search.best_score_:.4f}\")\n",
    "knn_removing_feature_results = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"knn removing dependent feature\", \"best score\": float(knn_grid_search.best_score_),'hyperparameter':knn_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322656f3-10cc-4e89-b015-0e6050f8681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of decision_tree algoritem in order to find the best hyperparameters after removing feature with high correlation\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  \n",
    "    'max_depth': [3, 5, 10, None],     \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4]    \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring=f1)\n",
    "decision_tree_grid_search.fit(df_removed_feature, y_train)\n",
    "print(f\"Best decision tree Hyperparameters: {decision_tree_grid_search.best_params_}\")\n",
    "best_model = decision_tree_grid_search.best_estimator_\n",
    "test_f1_score = f1_score(y_train, best_model.predict(df_removed_feature), average='weighted')\n",
    "print(f\"Best decision tree F1 Score: {test_f1_score:.4f}\")\n",
    "decision_tree_removing_feature_results = pd.DataFrame(decision_tree_grid_search.cv_results_)\n",
    "dict_ = {'model': \"decision tree removing dependent feature\", \"best score\": float(test_f1_score),'hyperparameter':decision_tree_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d09ef-d5f1-4873-b4f4-e04e1d130a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of NB algoritem in order to find the best hyperparameters after removing feature with high correlation\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "model = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=5)\n",
    "NB_grid_search.fit(df_removed_feature, y_train)\n",
    "print(f\"Best NB Hyperparameters: {NB_grid_search.best_params_}\")\n",
    "print(f\"Best NB F1 Score: {NB_grid_search.best_score_:.4f}\")\n",
    "NB_removing_feature_results = pd.DataFrame(NB_grid_search.cv_results_)\n",
    "dict_ = {'model': \"NB removing dependent feature\", \"best score\": float(NB_grid_search.best_score_),'hyperparameter':NB_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba169845-a769-4f73-ba8a-fa878bc57aa7",
   "metadata": {},
   "source": [
    "# *This is scaling, remove outliers and dependent feature results:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddf40c-e541-46ac-9fa6-77dfaaa75cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed_feature = filtered_df.drop(columns=['total_phenols']).copy()\n",
    "X_filtered = df_removed_feature.drop(\"target\",axis=1)\n",
    "y_train = df_removed_feature['target']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9ea68-3e00-4521-ab55-4233f327c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of knn algoritem in order to find the best hyperparameters after scaling, remove outliers and dependent feature\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],   \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'metric': ['euclidean', 'manhattan','minkowski','chebyshev']  \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring=f1)\n",
    "knn_grid_search.fit(X_scaled, y_train)\n",
    "# Best hyperparameters and F1 score for KNN\n",
    "print(\"Best KNN Hyperparameters:\", knn_grid_search.best_params_)\n",
    "print(f\"Best KNN F1 Score: {knn_grid_search.best_score_:.4f}\")\n",
    "knn_results = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "dict_ = {'model': \"knn all permutations\", \"best score\": float(knn_grid_search.best_score_),'hyperparameter':knn_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8d15a-9d21-4935-9cfa-12ee13600ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of decision_tree algoritem in order to find the best hyperparameters after remove outliers and dependent feature\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  \n",
    "    'max_depth': [3, 5, 10, None],    \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4]   \n",
    "}\n",
    "f1 = make_scorer(f1_score, average='weighted')\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=5, scoring=f1)\n",
    "decision_tree_grid_search.fit(X_filtered, y_train)\n",
    "print(f\"Best decision tree Hyperparameters: {decision_tree_grid_search.best_params_}\")\n",
    "best_model = decision_tree_grid_search.best_estimator_\n",
    "test_f1_score = f1_score(y_train, best_model.predict(X_filtered), average='weighted')\n",
    "print(f\"Best decision tree F1 Score: {test_f1_score:.4f}\")\n",
    "decision_tree_results = pd.DataFrame(decision_tree_grid_search.cv_results_)\n",
    "\n",
    "dict_ = {'model': \"decision tree 2 permutations\", \"best score\": float(test_f1_score),'hyperparameter':decision_tree_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b1f1f-b21d-4fc9-803e-dfb67885bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validitaion tranning of NB algoritem in order to find the best hyperparameters after scaling, remove outliers and dependent feature\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "model = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=5)\n",
    "NB_grid_search.fit(X_scaled, y_train)\n",
    "print(f\"Best NB Hyperparameters: {NB_grid_search.best_params_}\")\n",
    "print(f\"Best NB F1 Score: {NB_grid_search.best_score_:.4f}\")\n",
    "NB_results = pd.DataFrame(NB_grid_search.cv_results_)\n",
    "dict_ = {'model': \"NB all permutations\", \"best score\": float(NB_grid_search.best_score_),'hyperparameter':NB_grid_search.best_params_ }\n",
    "data.append(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0e0d2-ca5a-4c57-ae17-6c81c2911c24",
   "metadata": {},
   "source": [
    "# *Results comparison table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febd26c-997b-4545-a263-efe78fb44492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(data)\n",
    "df_results = df_results.sort_values(by='best score', ascending=False)\n",
    "df_results\n",
    "#The index number is according to the serial number of the model we ran.\n",
    "#It is affected by the sorting we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a968ad-d28c-4a91-b3e3-9c7dd16d792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0211f9-bc40-4bc5-8faf-eaf6e74f0115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcc4c8-d742-447f-8d98-8643353cd8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5ecdb-d76c-4cc2-ae36-da8fa2c3593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67d97f11",
   "metadata": {},
   "source": [
    "## Part 4 - Training \n",
    "Use the best combination of feature engineering, model (algorithm and hyperparameters) from the experiment part (part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84e9a7-7939-4389-9192-67958a601ca5",
   "metadata": {},
   "source": [
    "#decision tree without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb83228-7a89-4d91-a3b1-28f100000857",
   "metadata": {},
   "source": [
    "In the next section we will show that the first model with score equal 1 is not good because of overfitting.\n",
    "we will show that the model with the score closest to 1 (but not equal to 1) is the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93713ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traning the decision tree model without feature engineering\n",
    "#hyperparameter: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "\n",
    "X_train = df_train.drop(\"target\",axis=1)\n",
    "y_train = df_train['target']\n",
    "params = {\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2\n",
    "}\n",
    "dt_model = DecisionTreeClassifier(**params)\n",
    "dt_model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86187a71-6a8b-4db0-bd77-e6d1aef1c494",
   "metadata": {},
   "source": [
    "Traning the model with closest score to one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traning the knn model with scaling, removing outliers and feature depended\n",
    "#hyperparameter: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'uniform'}\n",
    "\n",
    "df_removed_feature = filtered_df.drop(columns=['total_phenols']).copy()\n",
    "X_filtered = df_removed_feature.drop(\"target\",axis=1)\n",
    "y_train = df_removed_feature['target']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "params = {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'uniform'}\n",
    "knn_model = KNeighborsClassifier(**params)\n",
    "knn_model.fit(X_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ad0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259ab902",
   "metadata": {},
   "source": [
    "## Part 5 - Apply on test and show model performance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23556a14-4c26-4d15-b3d5-12d49083f761",
   "metadata": {},
   "source": [
    "prediction of decision tree model without feature engineering\n",
    "hyperparameter: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6ca19-f4e7-4276-8550-5dd1372e762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(\"target\",axis=1)\n",
    "y_test = df_test['target']\n",
    "\n",
    "\n",
    "y_pred = dt_model.predict(X_test)\n",
    "#printing the report of model as you can see the model doesn't have good score- this is caused by the overfitting of the model traning.\n",
    "print(\"Classification Report:\")\n",
    "report = classification_report(y_test,y_pred, output_dict=True)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a28ca-03d8-4b39-a800-87aacfa73537",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = 1 - report['accuracy']\n",
    "print(f'error rate: {error_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df84b1-3995-4062-a05f-7d15de56c56c",
   "metadata": {},
   "source": [
    "# *The model that we choosed and his evaluation* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9971aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed_feature = df_test.drop(columns=['total_phenols']).copy()\n",
    "X_filtered = df_removed_feature.drop(\"target\",axis=1)\n",
    "y_test = df_test['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "y_pred = knn_model.predict(X_scaled)\n",
    "\n",
    "print('The first 5 predicted class results: ', y_pred[:5])\n",
    "print('The first 5 actual class results:    ', y_test.values[:5])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report = classification_report(y_test,y_pred, output_dict=True)\n",
    "print(classification_report(y_test, y_pred))\n",
    "error_rate = 1 - report['accuracy']\n",
    "print(f'error rate: {error_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_test[\"target\"].values, y_pred, labels=[0, 1, 2])\n",
    "confusion_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1', 'Actual 2'], columns=['Predicted 0', 'Predicted 1', 'Predicted 2'])\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82138643-c31f-44af-bb46-c7484bf9c2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
